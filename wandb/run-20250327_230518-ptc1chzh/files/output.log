You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM4-40GB MIG 3g.20gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name     | Type               | Params | Mode
---------------------------------------------------------
0  | conv1    | Conv2d             | 1.7 K  | train
1  | bn1      | BatchNorm2d        | 128    | train
2  | relu     | ReLU               | 0      | train
3  | layer1   | Sequential         | 147 K  | train
4  | layer2   | Sequential         | 525 K  | train
5  | layer3   | Sequential         | 2.1 M  | train
6  | layer4   | Sequential         | 8.4 M  | train
7  | avgpool  | AdaptiveAvgPool2d  | 0      | train
8  | fc       | Linear             | 161 K  | train
9  | loss_fn  | CrossEntropyLoss   | 0      | train
10 | accuracy | MulticlassAccuracy | 0      | train
---------------------------------------------------------
11.3 M    Trainable params
0         Non-trainable params
11.3 M    Total params
45.322    Total estimated model params size (MB)
68        Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 19: 100%|██████████| 982/982 [02:23<00:00,  6.86it/s, v_num=chzh, train_loss_step=8.470, train_acc_step=0.000, val_loss=0.795, val_acc=0.817, train_loss_epoch=0.138, train_acc_epoch=0.960]  
/gpfs/home1/scur0895/snellius-final-assignment/my_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=8` in the `DataLoader` to improve performance.
/gpfs/home1/scur0895/snellius-final-assignment/my_venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=8` in the `DataLoader` to improve performance.
                                                                          
`Trainer.fit` stopped: `max_epochs=20` reached.
